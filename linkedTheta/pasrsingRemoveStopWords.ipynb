{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malt import MaltParser # source code from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT COLLAPS THE PREPOSITION RELATIONSHIP\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "#from datasets import load_dataset\n",
    "from gensim import utils\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "\n",
    "from word import Word\n",
    "# from malt.malt import MaltParser # source code from nltk library\n",
    "from malt import MaltParser # source code from nltk library\n",
    "\n",
    "def process_dataset_removeStop_dontCollaps(docs, stop_words, pre_words, malt_parser_version='../maltparser-1.7.2', model_version='engmalt.linear-1.7.mco'):\n",
    "    # initalize malt parser model\n",
    "    mp = MaltParser(malt_parser_version, model_version, tagger=nltk.pos_tag)\n",
    "\n",
    "    #Add more preprocessing:\n",
    "    # replace single smart quote with single straight quote, so as to catch stopword contractions\n",
    "    docs = [re.sub(\"[\\u2018\\u2019]\", \"'\", doc) for doc in docs] #replace qoute with regualar qoutations\n",
    "    #it removes the digits\n",
    "    # docs = [re.sub('\\d+', '', doc) for doc in docs] \n",
    "    docs = [re.sub('(\\/.*?\\.[\\w:]+)', '', doc) for doc in docs]\n",
    "    docs = [re.sub(r\"http\\S+\", '', doc) for doc in docs]\n",
    "    \n",
    "    # create <doc_idx, tokenized_sent> list of sents\n",
    "    sents = [\n",
    "        (i, nltk.word_tokenize(sent))\n",
    "        for i, doc in enumerate(docs)\n",
    "        for sent in nltk.sent_tokenize(utils.to_unicode(str(doc).lower())) # convert doc to lowercase, and sentence tokenized.\n",
    "    ]\n",
    "\n",
    "    # unzip list of tuples\n",
    "    doc_idxs, sents = zip(*sents)\n",
    "\n",
    "    # create parser <generator> and loop through parser to produce dependency tree for each sentence\n",
    "    parser = mp.parse_sents(sents, verbose=True)\n",
    "\n",
    "    # define valid word\n",
    "    stop_words.append('amp');stop_words.append('&amp');stop_words.append('&amp;')\n",
    "    valid_word = lambda word: not word in stop_words and word.isalpha() and len(word) > 2\n",
    "\n",
    "    # initalize dictionary for json output\n",
    "    docs_dict = {\n",
    "        'documents': dict((doc_idx, {'words': [], 'relns': [], 'arg2':[] , 'originaltext': []}) for doc_idx in doc_idxs),\n",
    "        #'documents': dict((doc_idx, {'words': [], 'relns': []}) for doc_idx in doc_idxs),\n",
    "        'vocab': [], \n",
    "        'vocab_relns': [],\n",
    "        'vocab_arg':[],\n",
    "    }\n",
    "    # initalize vocab variables as sets (no duplicates)\n",
    "    vocab = set()\n",
    "    vocab_relns = set()\n",
    "    vocab_arg = set()\n",
    "    vocab_arg_test, vocab_relns_test = set(), set()\n",
    "\n",
    "    i = 0\n",
    "    # loop through list iterators\n",
    "    for list_it in parser:\n",
    "        tree = next(list_it)\n",
    "        # check if valid tree, if not skip\n",
    "        try:\n",
    "            nodes = tree.nodes\n",
    "        except:\n",
    "            continue\n",
    "        #print(nodes)\n",
    "#         print('******000000*****')\n",
    "        word_relns_hash = defaultdict(list)\n",
    "        word_args_hash = defaultdict(list)\n",
    "        for word_idx in nodes:\n",
    "            if word_idx == 0: # skip first\n",
    "                continue\n",
    "#             if word_idx >= 13:\n",
    "#                 pass\n",
    "#             if nodes[word_idx]['word']=='persons.for':\n",
    "#                 print('bad documents 1 : ', nodes[word_idx])\n",
    "#                 return nodes[word_idx]\n",
    "            deps = nodes[word_idx]['deps']\n",
    "            \n",
    "#             if (nodes[word_idx]['word'] in stop_words):\n",
    "#                 continue\n",
    "            # check for valid dependency relation\n",
    "            if deps:\n",
    "                for reln, idxs in deps.items():\n",
    "                    for idx in idxs:\n",
    "#                         if reln =='prep':\n",
    "#                             #what if it has more than one deps? do function/stop words appear with more than one deps?@ZHILA I also need to check what if this prepositional term refers to several words what do do with that\n",
    "# #                             if valid_word(nodes[idx]['word']):\n",
    "#                             if nodes[idx]['word'] in prep_words:\n",
    "#                                 reln = 'prep'+'.'+nodes[idx]['word']\n",
    "#                                 try:\n",
    "#                                     if len(list(nodes[idx]['deps'].values()))==0:\n",
    "#                                         continue\n",
    "#                                     elif type(list(nodes[idx]['deps'].values())[0])==list:\n",
    "#                                         idx = list(nodes[idx]['deps'].values())[0][0]\n",
    "#                                         if (not valid_word(nodes[word_idx]['word']) or (not valid_word(nodes[idx]['word']))):\n",
    "#                                             continue\n",
    "#                                     else:\n",
    "#                                         idx = list(nodes[idx]['deps'].values())[0]\n",
    "#                                         if (not valid_word(nodes[word_idx]['word']) or (not valid_word(nodes[idx]['word']))):\n",
    "#                                             continue\n",
    "#                                 except:\n",
    "#                                     print('Error')\n",
    "#                                     return (nodes, idx)\n",
    "                            \n",
    "#                             else:\n",
    "#                                 continue\n",
    "                            \n",
    "                        \n",
    "                        if (not valid_word(nodes[word_idx]['word']) or (not valid_word(nodes[idx]['word']))):\n",
    "                            continue\n",
    "\n",
    "                        dep_reln, gov_reln = f\"{reln}.dep\", f\"{reln}.gov\"\n",
    "                        if gov_reln ==\"punct.gov\":\n",
    "                            continue\n",
    "                        \n",
    "                        \n",
    "                        # add reln to word in hashmap\n",
    "                        word_relns_hash[idx].append(dep_reln) # append to dep word\n",
    "                        word_relns_hash[word_idx].append(gov_reln) # append to current word\n",
    "#                         if nodes[word_idx]['word'] == 'and' or nodes[idx]['word'] == 'and':\n",
    "#                             print('HT@')\n",
    "                        word_args_hash[idx].append(nodes[word_idx]['word'])\n",
    "                        word_args_hash[word_idx].append(nodes[idx]['word'])\n",
    "                        #vocab_arg.add(nodes[idx]['word'])\n",
    "                        #vocab_arg.add(nodes[word_idx]['word'])\n",
    "                        #vocab_relns.add(dep_reln)\n",
    "                        #vocab_relns.add(gov_reln)\n",
    "                        \n",
    "                        \n",
    "        \n",
    "        # check for valid hashmap\n",
    "        if word_relns_hash:\n",
    "            doc_idx = doc_idxs[i]\n",
    "\n",
    "            # loop through hashmap items and append to dict for future storing\n",
    "            for word_idx, relns in word_relns_hash.items():\n",
    "                word = nodes[word_idx]['word']\n",
    "                #relns = [reln for reln in relns if reln != \"punct.gov\"]\n",
    "#                 if word =='persons.for':\n",
    "#                     print(' some error in bad document: ', doc_idx)\n",
    "#                     return doc_idx\n",
    "                if valid_word(word):\n",
    "                    vocab.add(word)\n",
    "                    docs_dict['documents'][doc_idx]['words'].append(word)\n",
    "                    docs_dict['documents'][doc_idx]['relns'].append(relns)\n",
    "                    docs_dict['documents'][doc_idx]['originaltext'] = [docs[doc_idx]]\n",
    "                    docs_dict['documents'][doc_idx]['arg2'].append(word_args_hash[word_idx])\n",
    "                    try:\n",
    "                        vocab_arg.update(word_args_hash[word_idx])\n",
    "                        vocab_relns.update(relns)\n",
    "                    except:\n",
    "                        print(word_args_hash[word_idx])\n",
    "#                         print(vocab_arg_test)\n",
    "                        return 0\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    docs_dict['vocab'] = list(vocab)\n",
    "    docs_dict['vocab_relns'] = list(vocab_relns)\n",
    "    docs_dict['vocab_arg'] = list(set(vocab_arg))\n",
    "#     docs_dict['vocab_arg_test']=list(vocab_arg_test)\n",
    "#     docs_dict['vocab_relns_test']= list(vocab_relns_test)\n",
    "\n",
    "    return docs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the stop words\n",
    "import pickle\n",
    "# with open(\"../Stopword_list\",'rb') as read_file:\n",
    "with open('extra_stopwords','rb') as read_file:\n",
    "    more_stop_words = pickle.load(read_file)\n",
    "# stop word initialization\n",
    "# with open(\"data/utils/stopwords.txt\") as f:\n",
    "#     more_stop_words = f.read().splitlines()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(more_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('prep_words.txt', \"r\") as file:\n",
    "    prep_words = [line.strip() for line in file]\n",
    "# print(prep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "sql_db = mysql.connector.connect(host='127.0.0.1',user = 'covidAnalysis',password = 'k34p63MbDDcZ9yf4',\n",
    "                                 database = 'covid19framing')\n",
    "cursor = sql_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3699"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('show tables;')\n",
    "results = cursor.fetchall()\n",
    "# I changed the chosen column to be text_preproc and not text because of some issues in those rows wherein text_preproc is null and text exist...\n",
    "cursor.execute(\"select * from articles where text_preproc is not NULL\")\n",
    "results = cursor.fetchall()\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaltext = [item[1].decode() for item in results] #originaltextoriginaltext[1]\n",
    "corpus = pd.Series(originaltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = int(len(corpus) * .1)\n",
    "sample_corpus = corpus.sample(n=sample_size)\n",
    "len(sample_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "                          MaltParser 1.7.2                             \n",
      "-----------------------------------------------------------------------------\n",
      "         MALT (Models and Algorithms for Language Technology) Group          \n",
      "             Vaxjo University and Uppsala University                         \n",
      "                             Sweden                                          \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Started: Fri Aug 04 17:10:26 EDT 2023\n",
      "  Transition system    : Projective\n",
      "  Parser configuration : Stack\n",
      "  Feature model        : eng-liblinear.xml\n",
      "  Classifier           : liblinear\n",
      "  Data Format          : /engmalt.linear-1.7/conllx.xml\n",
      ".          \t      1\t      5s\t    365MB\n",
      ".          \t     10\t      5s\t    376MB\n",
      ".          \t    100\t      6s\t    464MB\n",
      "..........\t   1000\t     14s\t    780MB\n",
      "..........\t   2000\t     19s\t    527MB\n",
      "..........\t   3000\t     23s\t    385MB\n",
      "..........\t   4000\t     27s\t    647MB\n",
      "..........\t   5000\t     31s\t    454MB\n",
      "..         \t   5213\t     32s\t    625MB\n",
      "Parsing time: 00:00:30 (30607 ms)\n",
      "Finished: Fri Aug 04 17:10:58 EDT 2023\n",
      "/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/nltk/parse/dependencygraph.py:398: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs_dict = process_dataset(sample_corpus, stop_words, prep_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc = 'I thank the medical volunteers who have come forward, and I ask for others who have not yet raised their hand to help to do so now – your skills and support continues to be needed.'\n",
    "# d_test = pd.Series(test_doc)\n",
    "# tst= process_dataset(d_test, stop_words, pre_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_word = lambda word: not word in stop_words and word.isalpha() and len(word) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argupdates = docs_dict['vocab_arg']\n",
    "len(argupdates)\n",
    "'the' in argupdates, valid_word('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rels = docs_dict['vocab_relns']\n",
    "len(rels)\n",
    "'prep.for.dep' in rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(not valid_word('and') or (not valid_word('or')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = json.dumps(docs_dict, indent=4)\n",
    "with open(\"sample_10_without_stopwords.json\", \"w\") as f:\n",
    "    f.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sample_10.json\") as json_file:\n",
    "#     o = json.load(json_file)\n",
    "# arg2 =o['vocab_arg']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
