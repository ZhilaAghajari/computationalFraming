{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is perhaps only for LDA model because for LDA-GR and latent theta role we will need the original text.. at some point maybe we call this pre-processed data for them if the sample is too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define extra stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the stop words and add them to the list of extra stop words..\n",
    "extra_stop_words = open('Stopword_list', 'r')\n",
    "#addiong some stop words...\n",
    "extra_stop_words_list = extra_stop_words.readlines()\n",
    "extra_stopwords = set()\n",
    "for item in extra_stop_words_list:\n",
    "    extra_stopwords.add(item.strip())\n",
    "extra_stopwords.add('amp');extra_stopwords.add('&amp');extra_stopwords.add('&amp;')\n",
    "\n",
    "# print(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zha219/.local/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('wiki_vocab_dict','rb') as read_file:\n",
    "    wiki_vocab_dict = pickle.load(read_file)\n",
    "\n",
    "with open('vocab_dict', 'rb') as read_file:\n",
    "    vocab_dict = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(vocab_dict.token2id.keys())\n",
    "extra_stopwords = extra_stopwords.union(set(vocab_dict.token2id.keys()).difference(set(wiki_vocab_dict.token2id.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('extra_stopwords','wb') as save_file:\n",
    "    pickle.dump(extra_stopwords,save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zha219/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zha219/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zha219/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zha219/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#pre-processing \n",
    "#this part uses Amin's code \n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import similarities\n",
    "\n",
    "import csv, sys\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "try:\n",
    "\tcsv.field_size_limit(sys.maxsize)\n",
    "except:\n",
    "\tprint('Error in setting maxSize for CSV output')\n",
    "    \n",
    "def preprocess_data(doc_list, id_list, extra_stopwords = {},len_th=4,lemmatized=False):\n",
    "\t'''\n",
    "\tReturns: a list of process dataset and origianl documents of those documents\n",
    "\n",
    "\tThis function removes stop-wrods, lemmatized the documens, if stated, and eliminates the documnets \n",
    "\twith lenhgth of 4 or less. \n",
    "\t***These processes may result in lower number of documents than the original number. To make sure \n",
    "\tyou receive both the original docs and the processed doc in similar order we return both.\n",
    "\n",
    "\tparameter doc_list: a list of string (documents)\n",
    "\tparameter extra_stopwords: NLTK.stop_words are used, if you wish to add to that list, you can use yours.\n",
    "\tparameter len_th: documents with len_th and less will be removed.\n",
    "\tparameter lemmatized: If true, the terms will be lemmatized. **be aware that lemmatization of the documents\n",
    "\twill result in different topics and may need different evaluation, including NPMI, stability, or human assessment**\n",
    "\n",
    "\t'''\n",
    "\n",
    "\t# replace single smart quote with single straight quote, so as to catch stopword contractions\n",
    "\tdoc_list = [re.sub(\"[\\u2018\\u2019]\", \"'\", doc) for doc in doc_list] #replace qoute with regualar qoutations\n",
    "\tdoc_list = [re.sub('\\d+', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub('(\\/.*?\\.[\\w:]+)', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('pdf|icon|jpg', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub(r\"http\\S+\", '', doc) for doc in doc_list]\n",
    "\n",
    "\t# initialize regex tokenizer\n",
    "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t# create English stop words list\n",
    "\ten_stop = set(stopwords.words('english'))\n",
    "\t# add any extra stopwords\n",
    "\tif (len(extra_stopwords) > 0):\n",
    "\t\ten_stop = en_stop.union(extra_stopwords)\n",
    "\n",
    "\t#defining a lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\t# list for tokenized documents in loop\n",
    "\ttexts = []\n",
    "\toriginal_docs = []\n",
    "\t# loop through document list\n",
    "\tc = 0 #counter on the document number\n",
    "\tfor i in doc_list:\n",
    "\t\t# clean and tokenize document string\n",
    "\t\traw = i.lower()\n",
    "\t\ttokens = tokenizer.tokenize(raw)\n",
    "\t\tstopped_tokens = []\n",
    "\t\t# remove stop words from tokens\n",
    "\t\t#stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\tif lemmatized:\n",
    "\t\t  for t in tokens:\n",
    "\t\t    if t not in en_stop and len(t)>1:\n",
    "\t\t      pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([t])[0][1])\n",
    "\t\t      if pos:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t,pos=pos))\n",
    "\t\t      else:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t))\n",
    "\t\t  #     print(t,pos,nltk.pos_tag([t])[0][1])\n",
    "\t\t  # print(stopped_tokens)\n",
    "\t\t  #stopped_tokens = [lemmatizer.lemmatize(i,pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([i])[0][1])) for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\telse:\n",
    "\t\t  stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\n",
    "\n",
    "\t\t# add tokens to list\n",
    "\t\tif len(stopped_tokens) >= len_th:\n",
    "\t\t  texts.append([stopped_tokens, i, id_list[c]]) # [pre-processed text (tokenized), original, id]\n",
    "\t\t  #original_docs.append([i,c])\n",
    "\n",
    "\t\tc += 1\n",
    "\n",
    "\treturn texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "2013: Lost connection to MySQL server during query",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/zha219/9977172/ipykernel_1917863/2689879981.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmysql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sql_db = mysql.connector.connect(host='127.0.0.1',user = 'covidAnalysis',password = 'k34p63MbDDcZ9yf4',\n\u001b[0m\u001b[1;32m      4\u001b[0m                                  database = 'covid19framing')\n\u001b[1;32m      5\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0mConnect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m  \u001b[0;31m# pylint: disable=C0103\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/abstracts.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         self._do_auth(self._user, self._password,\n\u001b[1;32m    209\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_flags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_do_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;34m\"\"\"Get the handshake from the MySQL server\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mpacket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mrecv_plain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpacket_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterfaceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2013\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mpacket\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mpacket_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInterfaceError\u001b[0m: 2013: Lost connection to MySQL server during query"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "sql_db = mysql.connector.connect(host='127.0.0.1',user = 'covidAnalysis',password = 'k34p63MbDDcZ9yf4',\n",
    "                                 database = 'covid19framing')\n",
    "cursor = sql_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('show tables;')\n",
    "results = cursor.fetchall()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('desc twitter')\n",
    "#cursor.execute('desc twitter_hashtag')\n",
    "# cursor.execute('desc articles')\n",
    "results = cursor.fetchall()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch data and store back to the databse the pre-processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursorUpdate = sql_db.cursor()\n",
    "offset = 0#counter for offset\n",
    "results = [0] #starting with a non-empty results to make sure we get into loop in the first place\n",
    "\n",
    "while results:\n",
    "    cursor.execute(\"select * from twitter where text_preproc is NULL limit 1000 offset {0};\".format(offset))\n",
    "    #cursor.execute(\"select * from articles where text_preproc is NULL limit 1000 offset {0};\".format(offset))\n",
    "    offset += 1000\n",
    "    results = cursor.fetchall()\n",
    "    if not results:  # Check if there are no more results\n",
    "        break\n",
    "    result_dic = {i[0]:i for i in results}\n",
    "#     print(type([i[0] for i in results]), type([i[1].decode() for i in results]))\n",
    "    \n",
    "#     print(results[0])1\n",
    "    #send to pre_process (...)\n",
    "    out = preprocess_data([i[3].decode() for i in results], [i[0] for i in results], extra_stopwords,len_th=4,lemmatized=False)\n",
    "    #out ==> 1. pre_prcesed doc, original, id\n",
    "#     print(type(out))\n",
    "    #print(out)\n",
    "    #putting it back to DB\n",
    "    for o in out:\n",
    "        cursorUpdate.execute(\"update twitter SET text_preproc = '{0}' where id = {1}\".format(' '.join(o[0]),o[2]))\n",
    "        #print(o[0], o[2])\n",
    "    #cursor.fetchall()\n",
    "    #offset += 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0][-1].decode().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o[0], o[1], o[2]\n",
    "for o in out:\n",
    "    cursor.execute(\"update twitter SET text_preproc = '{0}' where id = {1}\".format(' '.join(o[0]),o[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in out:\n",
    "    cursor.execute(\"update twitter SET text_preproc = '{0}' where id = {1}\".format(' '.join(o[0]),o[2]))\n",
    "    print(o[0], o[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"select * from twitter where text_preproc is not NULL;\")\n",
    "res =  cursor.fetchall()\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit to save the changes in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "sql_db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
