{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Coherence Calculations*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For coherence calculation we need: Import all docs, get data from DB, wiki vocab and dict, stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sample from db, prepare corpus, construct doc term matrix, THEN compute coherence values and plot coherence plots, THEN run LDA-> then plot based on that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import similarities\n",
    "#ssh -L 3306:127.0.0.1:3306 -N -f lem325@eltanin.dept.lehigh.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libiraries\n",
    "import gensim\n",
    "\n",
    "#check Gensim version\n",
    "assert '3.8.2' <= gensim.__version__ <= '3.8.3', 'You must install Gensim 3.8.3 or 3.8.2 to be able to run LDA Mallet'\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "import platform #May not be needed but alas\n",
    "#checking OS\n",
    "if 'windows' in platform.system().lower():\n",
    "\tmallet_path = 'C:/mallet-2.0.8/bin/mallet'\n",
    "elif 'linux' in platform.system().lower():\n",
    "    import os\n",
    "    os.environ['MALLET_HOME'] = 'mallet-2.0.8/'\n",
    "    mallet_path = 'mallet-2.0.8/bin/mallet' # you should NOT need to change this\n",
    "#     os.environ['MALLET_HOME'] = '../mallet-2.0.8/'\n",
    "#     mallet_path = '../mallet-2.0.8/bin/mallet' # you should NOT need to change this \n",
    "else:\n",
    "\tmallet_path = 'mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lem325/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#pre-processing \n",
    "#this part uses Amin's code \n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import similarities\n",
    "\n",
    "\n",
    "\n",
    "import csv, sys\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "try:\n",
    "\tcsv.field_size_limit(sys.maxsize)\n",
    "except:\n",
    "\tprint('Error in setting maxSize for CSV output')\n",
    "    \n",
    "def preprocess_data(doc_list, id_list, extra_stopwords = {},len_th=4,lemmatized=False):\n",
    "\t'''\n",
    "\tReturns: a list of process dataset and origianl documents of those documents\n",
    "\n",
    "\tThis function removes stop-wrods, lemmatized the documens, if stated, and eliminates the documnets \n",
    "\twith lenhgth of 4 or less. \n",
    "\t***These processes may result in lower number of documents than the original number. To make sure \n",
    "\tyou receive both the original docs and the processed doc in similar order we return both.\n",
    "\n",
    "\tparameter doc_list: a list of string (documents)\n",
    "\tparameter extra_stopwords: NLTK.stop_words are used, if you wish to add to that list, you can use yours.\n",
    "\tparameter len_th: documents with len_th and less will be removed.\n",
    "\tparameter lemmatized: If true, the terms will be lemmatized. **be aware that lemmatization of the documents\n",
    "\twill result in different topics and may need different evaluation, including NPMI, stability, or human assessment**\n",
    "\n",
    "\t'''\n",
    "\n",
    "\t# replace single smart quote with single straight quote, so as to catch stopword contractions\n",
    "\tdoc_list = [re.sub(\"[\\u2018\\u2019]\", \"'\", doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub('\\d+', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub('(\\/.*?\\.[\\w:]+)', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('pdf|icon|jpg', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub(r\"http\\S+\", '', doc) for doc in doc_list]\n",
    "\n",
    "\t# initialize regex tokenizer\n",
    "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t# create English stop words list\n",
    "\ten_stop = set(stopwords.words('english'))\n",
    "\t# add any extra stopwords\n",
    "\tif (len(extra_stopwords) > 0):\n",
    "\t\ten_stop = en_stop.union(extra_stopwords)\n",
    "\n",
    "\t#defining a lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\t# list for tokenized documents in loop\n",
    "\ttexts = []\n",
    "\toriginal_docs = []\n",
    "\t# loop through document list\n",
    "\tc = 0 #counter on the document number\n",
    "\tfor i in doc_list:\n",
    "\t\t# clean and tokenize document string\n",
    "\t\traw = i.lower()\n",
    "\t\ttokens = tokenizer.tokenize(raw)\n",
    "\t\tstopped_tokens = []\n",
    "\t\t# remove stop words from tokens\n",
    "\t\t#stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\tif lemmatized:\n",
    "\t\t  for t in tokens:\n",
    "\t\t    if t not in en_stop and len(t)>1:\n",
    "\t\t      pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([t])[0][1])\n",
    "\t\t      if pos:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t,pos=pos))\n",
    "\t\t      else:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t))\n",
    "\t\t  #     print(t,pos,nltk.pos_tag([t])[0][1])\n",
    "\t\t  # print(stopped_tokens)\n",
    "\t\t  #stopped_tokens = [lemmatizer.lemmatize(i,pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([i])[0][1])) for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\telse:\n",
    "\t\t  stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\n",
    "\n",
    "\t\t# add tokens to list\n",
    "\t\tif len(stopped_tokens) >= len_th:\n",
    "\t\t  texts.append([stopped_tokens, i, id_list[c]]) # [pre-processed text (tokenized), original, id]\n",
    "\t\t  #original_docs.append([i,c])\n",
    "\n",
    "\t\tc += 1\n",
    "\n",
    "\treturn texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import similarities\n",
    "#ssh -L 3306:127.0.0.1:3306 -N -f lem325@eltanin.dept.lehigh.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "#read all the stop words and add them to the list of extra stop words..\n",
    "extra_stop_words = open('Stopword_list.txt', 'r')\n",
    "#addiong some stop words...\n",
    "extra_stop_words_list = extra_stop_words.readlines()\n",
    "extra_stopwords = set()\n",
    "for item in extra_stop_words_list:\n",
    "    extra_stopwords.add(item.strip())\n",
    "extra_stopwords.add('amp');extra_stopwords.add('&amp');extra_stopwords.add('&amp;')\n",
    "\n",
    "# print(extra_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the wiki vocab and dict for coherence and update stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wiki_vocab_dict','rb') as read_file:\n",
    "    wiki_vocab_dict = pickle.load(read_file)\n",
    "\n",
    "with open('vocab_dict', 'rb') as read_file:\n",
    "    vocab_dict = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(vocab_dict.token2id.keys())\n",
    "extra_stopwords = extra_stopwords.union(set(vocab_dict.token2id.keys()).difference(set(wiki_vocab_dict.token2id.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import docs aka access database and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put code to grab from db here:\n",
    "#ssh -L 3306:127.0.0.1:3306 -N -f lem325@eltanin.dept.lehigh.edu\n",
    "\n",
    "#Read orig text and preproc from SQL db\n",
    "#CONNECT TO SQL\n",
    "#import mysql\n",
    "\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "sql_db = MySQLdb.connect(host='127.0.0.1',user='covidAnalysis', password = \"k34p63MbDDcZ9yf4\",db='covid19framing')\n",
    "cursor = sql_db.cursor()\n",
    "\n",
    "#originaltext = [item[1].decode() for item in results]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"select * from articles where text is not NULL\")\n",
    "results = cursor.fetchall()\n",
    "len(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import random\n",
    "#SUBSAMPLE OF 5 percent\n",
    "random.seed(123456)\n",
    "sample_size = int(len(results)*.02)\n",
    "completedocssampled = sample(results,sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist = [item[-1].split() for item in completedocssampled] #preprocessed \n",
    "originaltext = [item[1] for item in completedocssampled] #originaltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completedocs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/lem325/10341948/ipykernel_3123384/1366371831.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcompletedocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'completedocs' is not defined"
     ]
    }
   ],
   "source": [
    "del completedocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the sample documents:\n",
    "#Lowkey optional I think\n",
    "with open('completedocssampled.obj', 'wb') as objwriter:\n",
    "    pickle.dump(completedocssampled, objwriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean,no_below=5,no_above=0.5):\n",
    "\t'''\n",
    "\tReutrns: A dictionary of the final set of terms and document-term frequency matrix\n",
    "\n",
    "\t# adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "\n",
    "\tparameter doc_clean: processed set of documents (type: string)\n",
    "\tparameter no_below: exclude words that only appear $no_below$ times or less in the whole corpus\n",
    "\tparameter no_above: any words included in more than $no_above$ percentage of documents will be excluded\n",
    "\n",
    "\t'''\n",
    "\t# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "\tdictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "\tdictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "\t# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "\tdoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\t# generate LDA model\n",
    "\treturn dictionary,doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean,no_below=5,no_above=0.5):\n",
    "\t'''\n",
    "\tReutrns: A dictionary of the final set of terms and document-term frequency matrix\n",
    "\n",
    "\t# adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "\n",
    "\tparameter doc_clean: processed set of documents (type: string)\n",
    "\tparameter no_below: exclude words that only appear $no_below$ times or less in the whole corpus\n",
    "\tparameter no_above: any words included in more than $no_above$ percentage of documents will be excluded\n",
    "\n",
    "\t'''\n",
    "\t# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "\tdictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "\tdictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "\t# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "\tdoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\t# generate LDA model\n",
    "\treturn dictionary,doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually calling prepare corpus here\n",
    "\n",
    "\n",
    "# if you need to subsample, you can subsample here:\n",
    "#N = int(len(doclist)/1000)\n",
    "vocab_dict, doc_term_matrix = prepare_corpus(doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, ref_dict=[], limit=25, start=5, step=5,threshold=0.10,runs = 1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    purity_values: Average purity for each run\n",
    "    contrast_values: Average of contrast for each run\n",
    "    df: DataFrame df inlcudes all results and number of topics associated with those results\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    coherence_std = []\n",
    "    model_list = []\n",
    "    purity_values = []\n",
    "    contrast_values = []\n",
    "    all_top_terms = []#this list will store all topics of all models as a list of top terms.\n",
    "    # so with two models one with 10 and one with 20, we will have 30 of top terms\n",
    "    top_terms_count_ls = [0] #this list keeps track of topics we add in each run and for each num_topics\n",
    "    df = pd.DataFrame(columns=['num_topics','coherence','purity','contrast'])\n",
    "    for num_topics in range(start, limit+1, step):\n",
    "      model_t = []\n",
    "      purity_t = []\n",
    "      coherence_t = []\n",
    "      contrast_t = []\n",
    "      for r in range(runs):\n",
    "          #model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "          model = LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary,optimize_interval = 25,iterations=2000, workers = 6) # only add it on server and not local \n",
    "          #model_t.append(model)\n",
    "\n",
    "          #storing top_terms\n",
    "          for tn in range(num_topics): \n",
    "          \ttt = model.show_topic(tn,topn=20)\n",
    "          \t#saving top_terms and their counts\n",
    "          \tall_top_terms.append([i[0] for i in tt])\n",
    "\t\t\t\t  #saving counts of top_terms\n",
    "          top_terms_count_ls.append(len(all_top_terms))\n",
    "\n",
    "          #purity computation\n",
    "#           topic_term_cond = get_conditional_probabilities(model,num_topics)\n",
    "          topic_term = model.get_topics()#model.show_topics(num_topics = num_topics, num_words=200)\n",
    "          #go over each topic and compute set of words that satisfy p(t|w)>=threshold\n",
    "          pur = []\n",
    "          cont = []\n",
    "          for t in range(num_topics):\n",
    "            pur.append(0.0)\n",
    "            cont.append(0.0)\n",
    "#             w_ind = np.argwhere(topic_term_cond[t,:]>=threshold)\n",
    "#             pur.append(np.sum(topic_term[t,w_ind]))\n",
    "#             cont.append(np.mean(topic_term_cond[t,w_ind]))\n",
    "          \n",
    "          #for n in range(num_topics):\n",
    "            #pur.append(sum([float(i) for i in re.findall(r'\\d*\\.\\d+',topic_term[n][1]) if float(i)>=threshold]))\n",
    "\n",
    "          purity_t.append(np.mean(pur))\n",
    "          contrast_t.append(np.mean(cont))\n",
    "          # coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_npmi',processes=1)\n",
    "          # coherence_value = coherencemodel.get_coherence()\n",
    "          # coherence_t.append(coherence_value)\n",
    "          df = df.append({'num_topics':num_topics,'purity':np.mean(pur),'contrast':np.mean(cont)},ignore_index=True)\n",
    "\n",
    "        #stroing the results\n",
    "#       model_list.append(model)\n",
    "      purity_values.append(purity_t)\n",
    "      #coherence_values.append(coherence_t)\n",
    "      contrast_values.append(contrast_t)\n",
    "      print('{0} number of topics has been processed'.format(num_topics))\n",
    "\n",
    "    #computing coherence for all top terms at once\n",
    "    cscore = CoherenceModel(topics=all_top_terms,dictionary=ref_dict,texts=texts,coherence='c_npmi',processes=7).get_coherence_per_topic() #processes are the number of threads in computing coherence\n",
    "\n",
    "    for i in range(0,len(top_terms_count_ls)-1):\n",
    "    \tcoherence_values.append(np.mean(cscore[top_terms_count_ls[i]:top_terms_count_ls[i+1]]))\n",
    "    \tcoherence_std.append(np.std(cscore[top_terms_count_ls[i]:top_terms_count_ls[i+1]]))\n",
    "    \t#update dataframe\n",
    "    df.coherence = coherence_values\n",
    "    df['coherence_std'] = coherence_std\n",
    "    return model_list, coherence_values, purity_values, contrast_values,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " #2.1 function to load wikipedia corpus\n",
    "def loading_wiki_docs(filename:str):\n",
    "\n",
    "    # Returns: documents of wikipedia corpus\n",
    "    # loads a wikipedia text file and return documents with length>3\n",
    "    # parameter filename: name of the wikipedia text documents (type:str)\n",
    "\n",
    "    wiki_docs = []\n",
    "\n",
    "    with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "        d = f.readline()\n",
    "        wiki_docs.append(d)\n",
    "    while(d):\n",
    "        d = f.readline()\n",
    "        if len(d) > 3:\n",
    "            wiki_docs.append(d)\n",
    "    return wiki_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../wiki_corpus/wiki_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/lem325/10501522/ipykernel_986803/676886998.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2.2 load wikipedia for coherence computing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#topic coherence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwiki_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloading_wiki_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../wiki_corpus/wiki_full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#doing pre-processing on wiki-pedia documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpre_processed_wiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/ceph/scratch/lem325/10501522/ipykernel_986803/201608366.py\u001b[0m in \u001b[0;36mloading_wiki_docs\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0mwiki_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m    \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m        \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[0mwiki_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../wiki_corpus/wiki_full'"
     ]
    }
   ],
   "source": [
    "#2.2 load wikipedia for coherence computing\n",
    "#topic coherence\n",
    "wiki_docs = loading_wiki_docs('../wiki_corpus/wiki_full')\n",
    "#doing pre-processing on wiki-pedia documents\n",
    "pre_processed_wiki = preprocess_data(wiki_docs,[0]*len(wiki_docs))\n",
    "pre_processed_wiki = [i[0] for i in pre_processed_wiki]\n",
    "wiki_vocab_dict, _ = prepare_corpus(pre_processed_wiki)\n",
    "del wiki_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('pre_processed_wiki','wb') as save_file:\n",
    "#     pickle.dump(pre_processed_wiki,save_file)\n",
    "        \n",
    "# with open('wiki_vocab_dict','wb') as save_file:\n",
    "#     pickle.dump(wiki_vocab_dict,save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impodoc_term_matrixe\n",
    "with open('pre_processed_wiki','rb') as read_file:\n",
    "    pre_processed_wiki = pickle.load(read_file)\n",
    "        \n",
    "with open('wiki_vocab_dict','rb') as read_file:\n",
    "    wiki_vocab_dict = pickle.load(read_file)\n",
    "\n",
    "#with open('doc_term_matrix', 'rb') as read_file:\n",
    "    #doc_term_matrix = pickle.load(read_file)\n",
    "\n",
    "with open('vocab_dict', 'rb') as read_file:\n",
    "    vocab_dict = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample wikipedia dictionary\n",
    "from random import sample\n",
    "import random\n",
    "random.seed(123456)\n",
    "wiki_sample_size = int(len(pre_processed_wiki)*.05)\n",
    "\n",
    "pre_processed_wiki = sample(pre_processed_wiki,wiki_sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265112"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_processed_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/scipy/cluster/_vq.cpython-38-x86_64-linux-gnu.so: failed to map segment from shared object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/lem325/10341948/ipykernel_3123384/2351219987.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiscplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/scipy/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hierarchy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/scipy/cluster/vq.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0m__docformat__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'restructuredtext'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages/scipy/cluster/_vq.cpython-38-x86_64-linux-gnu.so: failed to map segment from shared object"
     ]
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plotting_coherence(eval_df):\n",
    "    ax =sns.pointplot(x='num_topics', y='coherence', data=eval_df)\n",
    "    plt.title('Coherence score with Wiki docs as ref corpus')\n",
    "    plt.show()\n",
    "    ax = plt.errorbar(x='num_topics', y='coherence', yerr='coherence_std', data=eval_df[0:-1:3])\n",
    "    plt.title('Coherence score with std within a single run')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/lem325/10341948/ipykernel_3123384/3452098131.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_coherence_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_processed_wiki\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwiki_vocab_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#running on a VM machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# eval_df.to_csv('coherence_ap_10to90.csv',index=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/ceph/scratch/lem325/10341948/ipykernel_3123384/988612874.py\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, ref_dict, limit, start, step, threshold, runs)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;31m#model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLdaMallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimize_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only add it on server and not local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0;31m#model_t.append(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mserialize_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"serializing temporary corpus to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2mallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#3.run the LDA for the nuber of topics documents found above\n",
    "from random import sample\n",
    "import random\n",
    "random.seed(123456)\n",
    "lim = 40\n",
    "st = 40\n",
    "stp = 2\n",
    "n_runs = 2\n",
    "sample_size = 1000000\n",
    "models, coherence, pur, cont,eval_df = compute_coherence_values(dictionary=vocab_dict, corpus=doc_term_matrix, texts=pre_processed_wiki,ref_dict=wiki_vocab_dict, limit=lim, start=st, step=stp,threshold=0.10,runs = n_runs)\n",
    "#running on a VM machine\n",
    "# eval_df.to_csv('coherence_ap_10to90.csv',index=False)\n",
    "# plotting_coherence(eval_df)\n",
    "eval_df.to_csv('coherence_{0}to{1}_st{2}.csv'.format(st,lim,stp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/share/ceph/scratch/lem325/10341948/ipykernel_3123384/4266279041.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_df' is not defined"
     ]
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_coherence(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the ranges of coherence.\n",
    "\n",
    "coherence_10_90 = pd.read_csv('coherence_10to20_st20.csv')\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_30to50_st20.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_35to45_st10.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_55to55_st10.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_70to90_st20.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_36to54_st4.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_37to43_st2.csv'))\n",
    "coherence_10_90 = coherence_10_90.append(pd.read_csv('coherence_38to38_st2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_coherence(coherence_10_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_10_90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
