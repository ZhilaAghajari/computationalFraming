{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preprocessing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing, LDA end of day\n",
    "must log into this terminal eltanin every time\n",
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*READ ME:*\n",
    "*This file is to be used to preprocess data. How?* \n",
    "1) Reads data from mySQL and accesses original text data from file.\n",
    "2) Processes and stores related ID and documentList connected to original text.\n",
    "3) Updates original text, ID, and documentList attributes in mySQL database for usage in lda_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wheel in /share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages (0.35.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Cmake in /home/lem325/.local/lib/python3.8/site-packages (3.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim==3.8.3 in /home/lem325/.local/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages (from gensim==3.8.3) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/lem325/.local/lib/python3.8/site-packages (from gensim==3.8.3) (6.3.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages (from gensim==3.8.3) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /share/Apps/anaconda3/2020.07/envs/nlp/lib/python3.8/site-packages (from gensim==3.8.3) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wheel\n",
    "!pip install Cmake\n",
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: apt-get: command not found\n",
      "openjdk version \"1.8.0_322\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_322-b06)\n",
      "OpenJDK 64-Bit Server VM (build 25.322-b06, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def install_java():\n",
    "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
    "  !java -version       #check java version\n",
    "install_java()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "This section pertains to reading csv files, and storing them as dataframes, this section is not needed to be run anymore but helpful for future reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'\n",
    "mallet_path = '/content/mallet-2.0.8/bin/mallet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>public_metrics</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>qouted_id</th>\n",
       "      <th>replied_toid</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15754281</td>\n",
       "      <td>1586417588032053249</td>\n",
       "      <td>2022-10-29 18:00:01+00:00</td>\n",
       "      <td>{'urls': [{'start': 228, 'end': 251, 'url': 'h...</td>\n",
       "      <td>1586417588032053249</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 4, 'reply_count': 12, 'like_...</td>\n",
       "      <td>A first-of-its-kind survey measures a potentia...</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>[{'start': 228, 'end': 251, 'url': 'https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15754281</td>\n",
       "      <td>1585927131057356800</td>\n",
       "      <td>2022-10-28 09:31:08+00:00</td>\n",
       "      <td>{'urls': [{'start': 222, 'end': 245, 'url': 'h...</td>\n",
       "      <td>1585927134106615810</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 3, 'reply_count': 3, 'like_c...</td>\n",
       "      <td>For the past two-plus years, trick-or-treaters...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Null</td>\n",
       "      <td>1585927131057356800</td>\n",
       "      <td>[{'start': 222, 'end': 245, 'url': 'https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15754281</td>\n",
       "      <td>1585665123213000704</td>\n",
       "      <td>2022-10-27 16:10:00+00:00</td>\n",
       "      <td>{'urls': [{'start': 253, 'end': 276, 'url': 'h...</td>\n",
       "      <td>1585665123213000704</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 15, 'reply_count': 11, 'like...</td>\n",
       "      <td>Carnival Cruise Line has further eased its COV...</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>[{'start': 253, 'end': 276, 'url': 'https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15754281</td>\n",
       "      <td>1584844060535050242</td>\n",
       "      <td>2022-10-25 09:47:23+00:00</td>\n",
       "      <td>{'urls': [{'start': 198, 'end': 221, 'url': 'h...</td>\n",
       "      <td>1584844060535050242</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 13, 'reply_count': 5, 'like_...</td>\n",
       "      <td>In Tuesday's paper:\\n- Test scores plummet in ...</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>[{'start': 198, 'end': 221, 'url': 'https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15754281</td>\n",
       "      <td>1582476616160141331</td>\n",
       "      <td>2022-10-18 21:00:00+00:00</td>\n",
       "      <td>{'urls': [{'start': 192, 'end': 215, 'url': 'h...</td>\n",
       "      <td>1582476616160141331</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 17, 'reply_count': 6, 'like_...</td>\n",
       "      <td>BA.5 is losing its grip on the US, accounting ...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>Null</td>\n",
       "      <td>Null</td>\n",
       "      <td>[{'start': 192, 'end': 215, 'url': 'https://t....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_id      conversation_id                 created_at  \\\n",
       "0   15754281  1586417588032053249  2022-10-29 18:00:01+00:00   \n",
       "1   15754281  1585927131057356800  2022-10-28 09:31:08+00:00   \n",
       "2   15754281  1585665123213000704  2022-10-27 16:10:00+00:00   \n",
       "3   15754281  1584844060535050242  2022-10-25 09:47:23+00:00   \n",
       "4   15754281  1582476616160141331  2022-10-18 21:00:00+00:00   \n",
       "\n",
       "                                            entities                   id  \\\n",
       "0  {'urls': [{'start': 228, 'end': 251, 'url': 'h...  1586417588032053249   \n",
       "1  {'urls': [{'start': 222, 'end': 245, 'url': 'h...  1585927134106615810   \n",
       "2  {'urls': [{'start': 253, 'end': 276, 'url': 'h...  1585665123213000704   \n",
       "3  {'urls': [{'start': 198, 'end': 221, 'url': 'h...  1584844060535050242   \n",
       "4  {'urls': [{'start': 192, 'end': 215, 'url': 'h...  1582476616160141331   \n",
       "\n",
       "  lang                                     public_metrics  \\\n",
       "0   en  {'retweet_count': 4, 'reply_count': 12, 'like_...   \n",
       "1   en  {'retweet_count': 3, 'reply_count': 3, 'like_c...   \n",
       "2   en  {'retweet_count': 15, 'reply_count': 11, 'like...   \n",
       "3   en  {'retweet_count': 13, 'reply_count': 5, 'like_...   \n",
       "4   en  {'retweet_count': 17, 'reply_count': 6, 'like_...   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  A first-of-its-kind survey measures a potentia...              4   \n",
       "1  For the past two-plus years, trick-or-treaters...              3   \n",
       "2  Carnival Cruise Line has further eased its COV...             15   \n",
       "3  In Tuesday's paper:\\n- Test scores plummet in ...             13   \n",
       "4  BA.5 is losing its grip on the US, accounting ...             17   \n",
       "\n",
       "   like_count  reply_count  quote_count qouted_id         replied_toid  \\\n",
       "0          21           12            1      Null                 Null   \n",
       "1           9            3            0      Null  1585927131057356800   \n",
       "2          27           11            3      Null                 Null   \n",
       "3          30            5            2      Null                 Null   \n",
       "4          18            6            2      Null                 Null   \n",
       "\n",
       "                                                urls  \n",
       "0  [{'start': 228, 'end': 251, 'url': 'https://t....  \n",
       "1  [{'start': 222, 'end': 245, 'url': 'https://t....  \n",
       "2  [{'start': 253, 'end': 276, 'url': 'https://t....  \n",
       "3  [{'start': 198, 'end': 221, 'url': 'https://t....  \n",
       "4  [{'start': 192, 'end': 215, 'url': 'https://t....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Time to import from the CSV File, just kidding you need to import from mySql\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('tweets_USATODAY.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/lem325/.local/lib/python3.8/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set csv file here aka uncomment which you currently want to run\n",
    "myFile = \"tweets_USATODAY.csv\"\n",
    "#csvFile = \"tweets_nytimes.csv\"\n",
    "#csvFile = \"tweets_WSJ.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5931it [00:00, 43085.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#Import original documents (plain text)\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "completedocs = []\n",
    "originaltext = []\n",
    "with open(myFile, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in tqdm(csvreader):\n",
    "        originaltext.append(row[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5930it [00:00, 42262.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Import and pre process\n",
    "import csv\n",
    "with open(myFile,'r') as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            next(csvreader)\n",
    "            doclistNews, idlistNews = [], []\n",
    "            for row in tqdm(csvreader):\n",
    "                doclistNews.append(row[7])\n",
    "                idlistNews.append(row[4])\n",
    "            \n",
    "            #call the pre-processing\n",
    "            texts = preprocess_data(doclistNews, idlistNews, extra_stopwords = {},len_th=4,lemmatized=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5859"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "#len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQL Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to da server\n",
    "import pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "# To connect MySQL database\n",
    "sql_db = MySQLdb.connect(host='127.0.0.1',user='covidAnalysis', password = \"k34p63MbDDcZ9yf4\",db='covid19framing')\n",
    "cursor = sql_db.cursor()\n",
    "\n",
    "#cursor.execute('desc twitter;')\n",
    "#number_of_rows = cursor.execute(\"SELECT * FROM twitter\")\n",
    "#results = cursor.fetchall()\n",
    "\n",
    "    # To close the connection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('id', 'int(11)', 'NO', 'PRI', None, 'auto_increment')\n",
      "('text', 'mediumtext', 'NO', '', None, '')\n",
      "('date_published', 'timestamp', 'NO', '', 'CURRENT_TIMESTAMP', '')\n",
      "('url', 'mediumtext', 'NO', '', None, '')\n",
      "('title', 'varchar(1000)', 'NO', '', None, '')\n",
      "('source', 'varchar(60)', 'YES', '', None, '')\n",
      "('language', 'varchar(2)', 'YES', '', None, '')\n",
      "('date_scraped', 'timestamp', 'NO', '', 'CURRENT_TIMESTAMP', '')\n",
      "('state', 'varchar(14)', 'NO', '', None, '')\n",
      "('text_preproc', 'mediumtext', 'YES', '', None, '')\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('show tables;')\n",
    "#results = cursor.fetchall()\n",
    "\n",
    "\n",
    "cursor.execute('desc articles;')\n",
    "results = cursor.fetchall()\n",
    "\n",
    "for col in results:\n",
    "    print(col)\n",
    "\n",
    "\n",
    "#Visual SQL find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('id', 'int(11)', 'NO', 'PRI', None, 'auto_increment'), ('text', 'mediumtext', 'NO', '', None, ''), ('date_published', 'timestamp', 'NO', '', 'CURRENT_TIMESTAMP', ''), ('url', 'mediumtext', 'NO', '', None, ''), ('title', 'varchar(1000)', 'NO', '', None, ''), ('source', 'varchar(60)', 'YES', '', None, ''), ('language', 'varchar(2)', 'YES', '', None, ''), ('date_scraped', 'timestamp', 'NO', '', 'CURRENT_TIMESTAMP', ''), ('state', 'varchar(14)', 'NO', '', None, ''), ('text_preproc', 'mediumtext', 'YES', '', None, ''))\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "#print(\"Number of rows are:\" ,number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('show tables;')\n",
    "R = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('article_author',), ('article_keyword',), ('article_organization',), ('articles',), ('organizations',), ('twitter',), ('twitter_hashtag',))\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('article_keyword',)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor = sql_db.cursor()\n",
    "offset = 0#counter for offset\n",
    "TEST = [0] #starting with a non-empty results to make sure we get into loop in the first place\n",
    "\n",
    "while TEST:\n",
    "    #cursor.execute(\"SELECT * FROM articles WHERE text_preproc IS NOT NULL;\".format(offset))\n",
    "    cursor.execute(\"select * from twitter where text_preproc IS NOT NULL limit 100 offset {0};\".format(offset))\n",
    "    #cursor.execute(\"select * from articles where text_preproc is not NULL;\")\n",
    "    res =  cursor.fetchall()\n",
    "    #query = \"SELECT * FROM articles WHERE text_preproc IS NOT NULL AND text_preproc NOT IN ({}) LIMIT 1000 OFFSET %s;\".format(values_placeholder)\n",
    "    offset += 100\n",
    "    TEST = cursor.fetchall()\n",
    "    if not TEST:  # Check if there are no more results\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5532,\n",
       " None,\n",
       " '1322189888',\n",
       " '@TheAngryEpi @CDCgov I don’t know anyone with this option. As an “essential worker” we were abandoned by @CDCDirector and @WHCOVIDResponse and expected to repeatedly expose ourselves to positive #COVID19 cases with “tools” that didn’t protect us from infection and disability.',\n",
       " 24,\n",
       " 0,\n",
       " 2,\n",
       " datetime.datetime(2022, 10, 31, 23, 50, 58),\n",
       " datetime.datetime(2023, 3, 7, 10, 58, 11),\n",
       " 1587218683310403584,\n",
       " None,\n",
       " 1587218683310403584,\n",
       " 1587230681146527744,\n",
       " 'theangryepi cdcgov anyone option essential worker abandoned cdcdirector whcovidresponse expected repeatedly expose positive covid cases tools protect infection disability')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res[6]\n",
    "#print(\"Number of rows are:\" ,number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Stopword list\n",
    "#read all the stop words and add them to the list of extra stop words..\n",
    "extra_stop_words = open('Stopword_list.txt', 'r')\n",
    "#addiong some stop words...\n",
    "extra_stop_words_list = extra_stop_words.readlines()\n",
    "extra_stopwords = set()\n",
    "for item in extra_stop_words_list:\n",
    "    extra_stopwords.add(item.strip())\n",
    "extra_stopwords.add('amp');extra_stopwords.add('&amp');extra_stopwords.add('&amp;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version control set up\n",
    "# Get access to github repo and or branch.\n",
    "#organization_id col to separate\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "cursorUpdate = sql_db.cursor()\n",
    "offset = 0#counter for offset\n",
    "results = [0] #starting with a non-empty results to make sure we get into loop in the first place\n",
    "# Go through and fix each piece/evaluate \n",
    "\n",
    "while results:\n",
    "    cursor.execute(\"select * from twitter where text_preproc is NULL limit 1000 offset {0};\".format(offset))\n",
    "    offset += 1000\n",
    "    results = cursor.fetchall()\n",
    "    if not results:  # Check if there are no more results\n",
    "        break\n",
    "    result_dic = {i[0]:i for i in results}\n",
    "\n",
    "    #send to pre_process (...)\n",
    "    out = preprocess_data([i[3] for i in results], [i[0] for i in results], extra_stopwords,len_th=4,lemmatized=False)\n",
    "    #out ==> 1. pre_prcesed doc, original, id\n",
    "    \n",
    "    #putting it back to DB\n",
    "    for o in out:\n",
    "        cursor.execute(\"update twitter SET text_preproc = '{0}' where id = {1}\".format(' '.join(o[0]),o[2]))\n",
    "        print(o[0], o[2])\n",
    "                \n",
    "        \n",
    "sql_db.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lem325/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lem325/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#pre-processing \n",
    "#this part uses Amin's code \n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import similarities\n",
    "\n",
    "import csv, sys\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "try:\n",
    "\tcsv.field_size_limit(sys.maxsize)\n",
    "except:\n",
    "\tprint('Error in setting maxSize for CSV output')\n",
    "    \n",
    "def preprocess_data(doc_list, id_list, extra_stopwords = {},len_th=4,lemmatized=False):\n",
    "\t'''\n",
    "\tReturns: a list of process dataset and origianl documents of those documents\n",
    "\n",
    "\tThis function removes stop-wrods, lemmatized the documens, if stated, and eliminates the documnets \n",
    "\twith lenhgth of 4 or less. \n",
    "\t***These processes may result in lower number of documents than the original number. To make sure \n",
    "\tyou receive both the original docs and the processed doc in similar order we return both.\n",
    "\n",
    "\tparameter doc_list: a list of string (documents)\n",
    "\tparameter extra_stopwords: NLTK.stop_words are used, if you wish to add to that list, you can use yours.\n",
    "\tparameter len_th: documents with len_th and less will be removed.\n",
    "\tparameter lemmatized: If true, the terms will be lemmatized. **be aware that lemmatization of the documents\n",
    "\twill result in different topics and may need different evaluation, including NPMI, stability, or human assessment**\n",
    "\n",
    "\t'''\n",
    "\n",
    "\t# replace single smart quote with single straight quote, so as to catch stopword contractions\n",
    "\tdoc_list = [re.sub(\"[\\u2018\\u2019]\", \"'\", doc) for doc in doc_list] #replace qoute with regualar qoutations\n",
    "\tdoc_list = [re.sub('\\d+', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub('(\\/.*?\\.[\\w:]+)', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('pdf|icon|jpg', '', doc) for doc in doc_list]\n",
    "\t#doc_list = [re.sub('(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)', '', doc) for doc in doc_list]\n",
    "\tdoc_list = [re.sub(r\"http\\S+\", '', doc) for doc in doc_list]\n",
    "\n",
    "\t# initialize regex tokenizer\n",
    "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\t# create English stop words list\n",
    "\ten_stop = set(stopwords.words('english'))\n",
    "\t# add any extra stopwords\n",
    "\tif (len(extra_stopwords) > 0):\n",
    "\t\ten_stop = en_stop.union(extra_stopwords)\n",
    "\n",
    "\t#defining a lemmatizer\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\t# list for tokenized documents in loop\n",
    "\ttexts = []\n",
    "\toriginal_docs = []\n",
    "\t# loop through document list\n",
    "\tc = 0 #counter on the document number\n",
    "\tfor i in doc_list:\n",
    "\t\t# clean and tokenize document string\n",
    "\t\traw = i.lower()\n",
    "\t\ttokens = tokenizer.tokenize(raw)\n",
    "\t\tstopped_tokens = []\n",
    "\t\t# remove stop words from tokens\n",
    "\t\t#stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\tif lemmatized:\n",
    "\t\t  for t in tokens:\n",
    "\t\t    if t not in en_stop and len(t)>1:\n",
    "\t\t      pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([t])[0][1])\n",
    "\t\t      if pos:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t,pos=pos))\n",
    "\t\t      else:\n",
    "\t\t        stopped_tokens.append(lemmatizer.lemmatize(t))\n",
    "\t\t  #     print(t,pos,nltk.pos_tag([t])[0][1])\n",
    "\t\t  # print(stopped_tokens)\n",
    "\t\t  #stopped_tokens = [lemmatizer.lemmatize(i,pos=nltk_tag_to_wordnet_tag(nltk.pos_tag([i])[0][1])) for i in tokens if not i in en_stop and len(i)>1]\n",
    "\t\telse:\n",
    "\t\t  stopped_tokens = [i for i in tokens if not i in en_stop and len(i)>1]\n",
    "\n",
    "\n",
    "\t\t# add tokens to list\n",
    "\t\tif len(stopped_tokens) >= len_th:\n",
    "\t\t  texts.append([stopped_tokens, i, id_list[c]]) # [pre-processed text (tokenized), original, id]\n",
    "\t\t  #original_docs.append([i,c])\n",
    "\n",
    "\t\tc += 1\n",
    "\n",
    "\treturn texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
